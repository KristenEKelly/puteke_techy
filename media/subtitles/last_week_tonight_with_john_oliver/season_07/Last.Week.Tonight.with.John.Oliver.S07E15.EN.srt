1
00:00:23,060 --> 00:00:25,000
LAST WEEK TONIGHT
WITH JOHN OLIVER

2
00:00:25,080 --> 00:00:28,680
Hi there. Welcome to the show,
still taking place in this white,

3
00:00:28,760 --> 00:00:31,480
antiseptic room filled
with one increasingly weak adult.

4
00:00:31,600 --> 00:00:33,520
It's like a hospital, only sadder.

5
00:00:33,640 --> 00:00:36,840
It's been a week of continued
Black Lives Matter protests,

6
00:00:36,920 --> 00:00:40,400
which generated far-reaching
effects, both big and small.

7
00:00:40,520 --> 00:00:43,520
For instance, "Cops" was canceled
after 33 years of being

8
00:00:43,600 --> 00:00:45,760
"That Show Your Dad
Falls Asleep To."

9
00:00:46,080 --> 00:00:49,160
And "Sesame Street"
and CNN held a town hall to explain

10
00:00:49,280 --> 00:00:52,320
the current moment to children,
which didn't please all kids.

11
00:00:52,400 --> 00:00:55,160
In fact, just wait
for one giant baby's reaction.

12
00:00:55,640 --> 00:01:01,480
Across the country, people of color,
especially in the black community,

13
00:01:01,920 --> 00:01:05,440
are being treated unfairly
because of how they look.

14
00:01:07,280 --> 00:01:09,080
It's a children's show.
Got that, Bobby ?

15
00:01:09,200 --> 00:01:12,040
America is a very bad place
and it's your fault.

16
00:01:12,680 --> 00:01:15,960
So, no matter what they do
to you when you grow up,

17
00:01:16,080 --> 00:01:17,760
you have no right to complain.

18
00:01:18,160 --> 00:01:22,760
First: obviously, fuck off, Tucker,
you one-man homeowner's association.

19
00:01:23,120 --> 00:01:25,320
And second: that unspecified "they"

20
00:01:25,560 --> 00:01:27,760
in "what they do to you
when you grow up"

21
00:01:27,880 --> 00:01:30,120
is doing a lot of heavy lifting there.

22
00:01:30,360 --> 00:01:32,760
There's basically two options
for what that could mean.

23
00:01:32,840 --> 00:01:35,880
One: that Tucker and his viewers
have benefitted from a racist system

24
00:01:35,960 --> 00:01:39,000
that renders any specifications
of who "they" are unnecessary.

25
00:01:39,120 --> 00:01:41,920
Or two: that his show
is a badly-written piece of garbage.

26
00:01:42,040 --> 00:01:46,200
Which is it, Tucker ? Are you a racist
or are you a total fucking moron ?

27
00:01:46,360 --> 00:01:49,040
The answer can be,
and indeed is, both.

28
00:01:49,400 --> 00:01:52,320
But the momentum
wasn't just confined to television.

29
00:01:52,400 --> 00:01:55,920
Minneapolis City Council has pledged
to dismantle that city's police force,

30
00:01:56,000 --> 00:01:59,880
while New York lawmakers voted to
make police disciplinary records public

31
00:01:59,960 --> 00:02:03,600
and passed a law criminalizing
the use of chokeholds by police.

32
00:02:03,720 --> 00:02:07,800
Both of which seem like common sense
and the bare minimum,

33
00:02:07,880 --> 00:02:11,800
and yet, police union leaders
have continued to freak out.

34
00:02:12,440 --> 00:02:14,680
Stop treating us like animals
and thugs

35
00:02:15,320 --> 00:02:17,680
and start treating us
with some respect.

36
00:02:18,800 --> 00:02:20,800
That's what
we're here today to say.

37
00:02:21,240 --> 00:02:23,480
We've been left out
of the conversation.

38
00:02:24,440 --> 00:02:25,840
We've been vilified.

39
00:02:28,080 --> 00:02:30,760
It's disgusting.

40
00:02:31,480 --> 00:02:33,960
Yeah, you have been
left out of the conversation,

41
00:02:34,040 --> 00:02:37,880
but I'll tell you why: because you've
been fucking terrible at conversing.

42
00:02:38,080 --> 00:02:40,920
If a high school debate team
argued their rebuttals

43
00:02:41,000 --> 00:02:42,720
by tear gassing the other team,

44
00:02:42,800 --> 00:02:45,520
they probably wouldn't be
invited back to regionals.

45
00:02:45,640 --> 00:02:48,960
Meanwhile, some have taken
dismantling symbols of white supremacy

46
00:02:49,040 --> 00:02:50,280
into their own hands.

47
00:02:50,360 --> 00:02:52,880
Statues across the world
are being torn down.

48
00:02:52,960 --> 00:02:56,200
A slave trader in Bristol,
England was hurled into the harbor,

49
00:02:56,280 --> 00:03:00,320
King Leopold II in Belgium was spray
painted and ultimately removed

50
00:03:00,440 --> 00:03:03,800
and statues of Christopher Columbus
have been toppled, beheaded

51
00:03:03,880 --> 00:03:06,040
and, in one case,
thrown into a lake,

52
00:03:06,200 --> 00:03:11,040
where, and this is true, its location
was quickly updated on Google Maps.

53
00:03:11,240 --> 00:03:16,360
Unlike Christopher Columbus himself,
Google knows how to read a map.

54
00:03:16,800 --> 00:03:19,880
These protests have given momentum
to official campaigns,

55
00:03:19,960 --> 00:03:23,120
like Virginia's effort
to remove a statue of Robert E. Lee,

56
00:03:23,200 --> 00:03:26,960
an effort that some, like this
state senator, have been troubled by.

57
00:03:27,280 --> 00:03:31,840
It's all about shoving
this down people's throats

58
00:03:31,920 --> 00:03:36,040
and erasing the history
of the white people.

59
00:03:36,160 --> 00:03:37,840
And I think that's wrong.

60
00:03:39,080 --> 00:03:42,720
Setting aside her almost clinical
ability to miss the point,

61
00:03:42,800 --> 00:03:45,680
you cannot erase
the history of white people.

62
00:03:45,840 --> 00:03:48,680
It's like the skid marks on
the ass of your favorite shorts.

63
00:03:48,800 --> 00:03:52,200
No matter how hard you try,
that shit's never coming out.

64
00:03:52,680 --> 00:03:56,840
Statues aren't the only tributes to
a horrifying past being reconsidered.

65
00:03:57,040 --> 00:03:59,200
NASCAR banned
the Confederate flag,

66
00:03:59,280 --> 00:04:03,040
which prompted one driver to announce
he's quitting after this season.

67
00:04:03,120 --> 00:04:05,960
Although, to be fair, that driver
had never won a race,

68
00:04:06,040 --> 00:04:09,920
so you understand why a flag for losers
might have been important to him.

69
00:04:10,080 --> 00:04:14,080
The country group Lady Antebellum
changed their name to just Lady A.

70
00:04:14,240 --> 00:04:15,320
Which is a solid fix

71
00:04:15,440 --> 00:04:19,040
as long as nobody ever asks
what the "A" stands for

72
00:04:19,160 --> 00:04:22,800
or points out that that name's been
used by a black blues singer

73
00:04:22,880 --> 00:04:24,400
for the last 20 years.

74
00:04:24,720 --> 00:04:26,280
And then there was this.

75
00:04:27,000 --> 00:04:29,800
"Gone with the Wind"
is gone from HBO.

76
00:04:30,120 --> 00:04:32,680
It's one of the most popular
films of all time,

77
00:04:32,760 --> 00:04:36,320
but it's also been condemned
for ignoring the horrors of slavery.

78
00:04:36,600 --> 00:04:40,640
White House Press Secretary
Kayleigh McEnany tore into HBO,

79
00:04:40,720 --> 00:04:44,040
making it clear she was speaking
on behalf of the president.

80
00:04:44,360 --> 00:04:49,200
Where do you draw the line here ?
I'm told that no longer can you find

81
00:04:49,280 --> 00:04:53,240
on HBO "Gone with the Wind" because
somehow, that is now offensive.

82
00:04:53,600 --> 00:04:56,360
Where do you draw the line ?
Should George Washington,

83
00:04:56,480 --> 00:04:59,760
Thomas Jefferson and James Madison
be erased from history ?

84
00:05:00,200 --> 00:05:04,400
First: as I've said before, the answer
to: "Where do you draw the line ?"

85
00:05:04,480 --> 00:05:06,480
is literally always: somewhere.

86
00:05:06,560 --> 00:05:11,240
You draw it somewhere. HBO is not
permanently pulling the movie,

87
00:05:11,320 --> 00:05:13,720
it's going back up
with additional context.

88
00:05:13,800 --> 00:05:17,760
And finally: who gives a shit
if something's not on HBO Max ?

89
00:05:17,920 --> 00:05:20,800
There may be no better way
to obliterate all evidence

90
00:05:20,880 --> 00:05:24,400
of something's existence
than to put it on HBO Max,

91
00:05:24,480 --> 00:05:28,000
the only ash heap of history
that costs $15 a month.

92
00:05:28,320 --> 00:05:31,840
Obviously:
symbolic progress is progress.

93
00:05:31,920 --> 00:05:34,840
And a lot of these changes
have been a long time coming.

94
00:05:34,920 --> 00:05:37,800
But, this week also brought
stubborn reminders

95
00:05:37,880 --> 00:05:42,040
of the institutional inertia that is
going to make real change so difficult,

96
00:05:42,160 --> 00:05:47,440
like Joe Biden sticking by his plan
to invest an additional $300 million

97
00:05:47,520 --> 00:05:49,640
into community policing efforts,
which is an example

98
00:05:49,760 --> 00:05:52,840
of whatever the precise
opposite of reading the room is.

99
00:05:52,960 --> 00:05:58,160
We saw yet more footage of police
handcuffing black teens for jaywalking,

100
00:05:58,280 --> 00:06:01,280
as well as a newly released video
of Oklahoma officers

101
00:06:01,560 --> 00:06:04,080
responding to a man saying:
"I can't breathe"

102
00:06:04,160 --> 00:06:05,800
by saying: "I don't care."

103
00:06:06,160 --> 00:06:08,040
That man, incidentally, Derrick Scott,

104
00:06:08,120 --> 00:06:11,320
would go on to die at the hospital
of a collapsed lung.

105
00:06:11,960 --> 00:06:14,560
Perhaps most infuriatingly of all,

106
00:06:14,760 --> 00:06:18,400
as protestors continued to demand
justice for Breonna Taylor,

107
00:06:18,520 --> 00:06:22,400
who was killed in her own home
by police executing a noknock warrant,

108
00:06:22,760 --> 00:06:25,800
the Louisville Police Department
responded with this.

109
00:06:25,920 --> 00:06:28,360
Police in Louisville are releasing
the incident report

110
00:06:28,440 --> 00:06:31,560
from the night officers shot
and killed Breonna Taylor.

111
00:06:31,640 --> 00:06:35,000
As the "Courier-Journal" reports,
it's almost entirely blank.

112
00:06:35,080 --> 00:06:37,800
The four-page report
says Taylor had no injuries,

113
00:06:37,920 --> 00:06:40,040
even though police shot her
at least eight times.

114
00:06:40,520 --> 00:06:43,600
Holy shit.
That is appalling.

115
00:06:43,920 --> 00:06:48,280
When it comes to erasing history,
this seems a fuck of a lot worse

116
00:06:48,360 --> 00:06:51,360
than leaving a bunch of statues
toppled, cracked and beheaded.

117
00:06:51,480 --> 00:06:54,560
Or, as that would probably be described
on a Louisville police report:

118
00:06:54,680 --> 00:06:55,800
"No injuries."

119
00:06:56,120 --> 00:06:58,760
It's important that we deal honestly

120
00:06:58,840 --> 00:07:00,960
with the uncomfortable aspects
of our past.

121
00:07:01,080 --> 00:07:04,160
But, there's also hard,
necessary work to be done

122
00:07:04,240 --> 00:07:07,440
in changing the unacceptable
conditions of our present.

123
00:07:07,520 --> 00:07:09,600
And the only hope for that
is if,

124
00:07:09,680 --> 00:07:13,640
to say something that has never been
said about the offerings on HBO Max,

125
00:07:14,000 --> 00:07:16,640
people don't take
their fucking eyes off this.

126
00:07:17,160 --> 00:07:18,480
And now, this.

127
00:07:19,200 --> 00:07:23,640
C-SPAN Callers Have Some Thoughts
on the Coronavirus

128
00:07:23,920 --> 00:07:26,880
for the Second Most Patient Man
on Television.

129
00:07:27,560 --> 00:07:31,680
I only have one question.
I wanted to know who was

130
00:07:31,760 --> 00:07:38,720
the last politician or ex-politician
to visit China before this virus ?

131
00:07:40,080 --> 00:07:43,640
- And why it matters ?
- I'm curious.

132
00:07:45,440 --> 00:07:48,880
I've seen a guy walking down
through a grocery store

133
00:07:49,040 --> 00:07:51,600
with a jockey strap on his face.

134
00:07:51,920 --> 00:07:56,040
I practiced this physical distancing
long before anybody tried to tell me

135
00:07:56,240 --> 00:07:58,400
I have to,
'cause I don't like people.

136
00:07:58,800 --> 00:08:01,800
Sam is in Seattle, Washington.
Sam, go ahead.

137
00:08:03,480 --> 00:08:06,200
- So, how you doing ?
- Fine, thanks. Go ahead.

138
00:08:08,320 --> 00:08:09,760
What do you want to ask me ?

139
00:08:09,840 --> 00:08:13,040
Richard in Maryland, hello.

140
00:08:13,280 --> 00:08:16,560
I would just like to thank you
for taking my call, number one.

141
00:08:16,640 --> 00:08:18,400
And number two, you
should go fuck yourself.

142
00:08:18,520 --> 00:08:20,240
Who the hell is he ?
He's nobody.

143
00:08:20,360 --> 00:08:24,120
He's just a man from Brooklyn, Fauci.
Who cares about what he thinks ?

144
00:08:24,320 --> 00:08:28,520
He's the lead epidemiologist
on the White House task force team.

145
00:08:28,800 --> 00:08:31,680
Who is he ? We've got somebody
at the door. Who's that ?

146
00:08:32,040 --> 00:08:36,760
I hope you guys have a great day.
Stay safe and baba booey.

147
00:08:38,640 --> 00:08:42,160
Moving on. Our main story
concerns facial recognition,

148
00:08:42,280 --> 00:08:45,440
the thing that makes sure my iPhone
won't open unless it sees my face

149
00:08:45,520 --> 00:08:49,200
or the face of any toucan,
but, that is it.

150
00:08:49,640 --> 00:08:51,000
Facial recognition technology

151
00:08:51,080 --> 00:08:53,680
has been showcased in TV shows
and movies for years.

152
00:08:53,760 --> 00:08:56,320
Denzel Washington even discovered
a creative use for it

153
00:08:56,400 --> 00:08:58,640
in the 2006 action movie "Déja Vu".

154
00:08:58,840 --> 00:09:01,400
- We have facial recognition software ?
- Yeah.

155
00:09:01,520 --> 00:09:05,400
Let's use it on the bag. Cross-match it
to all the bags on the south side

156
00:09:05,480 --> 00:09:07,600
in the 48 hours
leading up to the explosion !

157
00:09:11,400 --> 00:09:13,640
Don't think
it's ever been used this way.

158
00:09:14,240 --> 00:09:15,320
Look, same bag.

159
00:09:16,200 --> 00:09:17,320
Bingo.

160
00:09:17,720 --> 00:09:19,640
Bingo, indeed, Denzel !

161
00:09:19,960 --> 00:09:22,280
With smart, believable plot
development like that,

162
00:09:22,360 --> 00:09:26,200
it's no wonder "Déja Vu" received
such glowing IMDB reviews as:

163
00:09:26,320 --> 00:09:29,160
"An insult to anybody
who finished elementary school",

164
00:09:29,280 --> 00:09:32,360
"Worst movie of all time",
"More like Deja Pooh."

165
00:09:32,480 --> 00:09:34,960
And my personal favorite:
a one-star review that reads:

166
00:09:35,080 --> 00:09:37,600
"Bruce Greenwood, as always,
is great and so sexy"

167
00:09:37,680 --> 00:09:39,360
"and there's a cat who survives."

168
00:09:39,440 --> 00:09:43,880
A review that was clearly written
either by Bruce Greenwood or that cat.

169
00:09:44,280 --> 00:09:48,000
Technology behind facial recognition
has been around for years.

170
00:09:48,080 --> 00:09:52,440
As it's grown more sophisticated,
its applications have expanded greatly.

171
00:09:52,600 --> 00:09:55,800
For instance, it's no longer just
humans who can be the targets.

172
00:09:56,400 --> 00:09:58,600
The iFarm sensor scans each fish

173
00:10:00,440 --> 00:10:05,720
and uses automatic image processing
to uniquely identify each individual.

174
00:10:06,040 --> 00:10:09,920
A number of symptoms are
recognized, including loser fish.

175
00:10:10,480 --> 00:10:12,080
Yes, "loser fish",

176
00:10:12,320 --> 00:10:14,840
which, by the way,
is an actual industry term.

177
00:10:14,920 --> 00:10:19,040
That company says it can detect
which fish are losers by facial scan.

178
00:10:19,120 --> 00:10:20,560
Which is important,
'cause, can you tell

179
00:10:20,640 --> 00:10:23,400
which one of these fish is a loser
and which one is a winner ?

180
00:10:23,480 --> 00:10:26,280
Are you sure about that ?
'Cause they're the same fish !

181
00:10:26,400 --> 00:10:28,120
This is why you need a computer !

182
00:10:28,400 --> 00:10:31,680
But the growth of facial recognition
and what it's capable of

183
00:10:31,760 --> 00:10:34,880
brings with it a host of privacy
and civil liberties issues.

184
00:10:34,960 --> 00:10:38,480
If you want a sense of how terrifying
this technology could be

185
00:10:38,560 --> 00:10:40,800
if it becomes part of everyday life,

186
00:10:41,080 --> 00:10:45,520
just watch as a Russian TV presenter
demonstrates an app called FindFace.

187
00:10:46,560 --> 00:10:49,400
If you find yourself in a café
with an attractive girl

188
00:10:49,520 --> 00:10:52,240
and you don't have
the guts to approach her, no problem.

189
00:10:52,560 --> 00:10:55,760
All you need is a smartphone
and the application FindFace.

190
00:10:56,160 --> 00:10:57,360
Find new friends,

191
00:11:00,720 --> 00:11:01,840
take a picture

192
00:11:05,200 --> 00:11:06,600
and wait for the result.

193
00:11:06,680 --> 00:11:09,280
Now you're already looking
at her profile page.

194
00:11:10,040 --> 00:11:11,240
Burn it all down.

195
00:11:11,600 --> 00:11:14,800
Burn. Everything. Down.
I realize that this is a sentence

196
00:11:14,880 --> 00:11:17,440
that no one involved in creating
that app ever once thought,

197
00:11:17,560 --> 00:11:20,240
but imagine that
from a woman's perspective.

198
00:11:20,560 --> 00:11:23,360
You're going about your day
when you get a random message

199
00:11:23,440 --> 00:11:27,080
from a guy you don't know:
"Hello, I saw you in cafe earlier,"

200
00:11:27,160 --> 00:11:30,720
"and used FindFace app to learn
your name and contact information."

201
00:11:30,840 --> 00:11:35,000
"I'll pick you up from your place
at eight. I know where you live."

202
00:11:35,560 --> 00:11:40,920
But one of the biggest users of
facial recognition is law enforcement.

203
00:11:41,200 --> 00:11:42,480
Since 2011,

204
00:11:42,640 --> 00:11:46,920
the FBI has logged more than
390 000 facial-recognition searches.

205
00:11:47,080 --> 00:11:49,560
The databases law enforcement
are pulling from include

206
00:11:49,640 --> 00:11:53,840
over 117 million American adults and
incorporate, among other things,

207
00:11:53,920 --> 00:11:57,480
drivers' license photos
from residents of all of these states.

208
00:11:57,800 --> 00:12:01,840
Roughly one in two of us have had
our photos searched this way.

209
00:12:01,960 --> 00:12:04,680
And the police will argue
that this is all for the best.

210
00:12:04,760 --> 00:12:08,440
An official with the London police
explaining why they use it there.

211
00:12:08,680 --> 00:12:11,000
In London we've had
the London Bridge attack,

212
00:12:11,080 --> 00:12:12,680
the Westminster Bridge attack.

213
00:12:12,760 --> 00:12:15,240
The suspects involved, the people
who were guilty of those offenses,

214
00:12:15,360 --> 00:12:17,040
were often known
by the authorities.

215
00:12:17,240 --> 00:12:18,880
Had they been on some database,

216
00:12:18,960 --> 00:12:21,040
had they been picked up
by cameras beforehand,

217
00:12:21,160 --> 00:12:23,520
we may have been able
to prevent those atrocities

218
00:12:23,600 --> 00:12:25,520
and that would definitely be a price
worth paying.

219
00:12:25,800 --> 00:12:29,400
It's hard to come out against
the prevention of atrocities.

220
00:12:29,520 --> 00:12:32,560
This show is, and always has been,
anti-atrocity.

221
00:12:32,920 --> 00:12:35,960
But the key question there is,
what's the trade-off ?

222
00:12:36,240 --> 00:12:38,760
If the police could guarantee
they could prevent all robberies,

223
00:12:38,840 --> 00:12:41,640
but the only way to do that is
by having an officer stationed

224
00:12:41,760 --> 00:12:44,400
in every bathroom watching you
every time you take a shit,

225
00:12:44,480 --> 00:12:47,360
I'm not sure everyone
would agree that it's worth it.

226
00:12:47,520 --> 00:12:48,560
The people who do

227
00:12:48,640 --> 00:12:51,560
might want that for reasons
other than preventing crime.

228
00:12:51,640 --> 00:12:54,760
Now is actually a very good time
to be looking at this issue.

229
00:12:54,880 --> 00:12:58,240
Because there are serious concerns
that facial recognition

230
00:12:58,320 --> 00:13:01,120
is being used to identify
Black Lives Matter protesters.

231
00:13:01,200 --> 00:13:04,480
And if that's true, it wouldn't
actually be the first time,

232
00:13:04,680 --> 00:13:08,280
as this senior scientist at Google,
Timnit Gebru, will tell you.

233
00:13:08,760 --> 00:13:13,920
There was an example with Baltimore
police and the Freddie Gray marches

234
00:13:14,040 --> 00:13:18,040
where they used face recognition
to identify protesters

235
00:13:18,560 --> 00:13:23,680
and then they tried to link them up
with their social media profiles

236
00:13:23,800 --> 00:13:25,680
and then target them for arrests.

237
00:13:25,800 --> 00:13:28,440
Right now, a lot of people
are urging people

238
00:13:28,520 --> 00:13:32,560
not to put images of protesters
on social media,

239
00:13:32,640 --> 00:13:34,280
'cause there are people out there

240
00:13:34,400 --> 00:13:37,840
whose job it is just to look up
these people and target them.

241
00:13:38,360 --> 00:13:41,120
It's true.
During the Freddie Gray protests,

242
00:13:41,200 --> 00:13:44,640
police officers used facial recognition
technology to look for people

243
00:13:44,720 --> 00:13:47,040
with outstanding warrants
and arrest them.

244
00:13:47,120 --> 00:13:50,480
Which is a pretty sinister way
to undermine the right to assemble.

245
00:13:50,720 --> 00:13:53,640
So tonight, let's take a look
at facial recognition.

246
00:13:53,720 --> 00:13:56,960
Let's start with the fact that even
as big companies like Microsoft,

247
00:13:57,040 --> 00:13:59,040
Amazon and IBM
have been developing it,

248
00:13:59,120 --> 00:14:02,760
and governments all over the world
have been happily rolling it out,

249
00:14:02,880 --> 00:14:07,280
there haven't been rules or a framework
in place for how it is used.

250
00:14:07,680 --> 00:14:11,040
In Britain, they've been experimenting
with "facial recognition zones",

251
00:14:11,120 --> 00:14:14,800
putting signs up alerting
that you're about to enter one.

252
00:14:14,880 --> 00:14:17,960
Which seems polite,
but watch what happens when one man

253
00:14:18,040 --> 00:14:20,560
decided he didn't actually
want his face scanned.

254
00:14:20,800 --> 00:14:23,240
This man didn't want to be
caught by the police cameras,

255
00:14:23,360 --> 00:14:24,680
so he covered his face.

256
00:14:24,760 --> 00:14:27,640
Police stopped him.
They photographed him anyway.

257
00:14:28,080 --> 00:14:29,320
An argument followed.

258
00:14:29,560 --> 00:14:31,000
What's your suspicion ?

259
00:14:31,480 --> 00:14:35,080
The fact that he walked past
clearly marked "facial recognition"

260
00:14:35,160 --> 00:14:36,640
and he covered his face.

261
00:14:37,040 --> 00:14:39,760
So, I walked past like that.
It's a cold day as well.

262
00:14:40,640 --> 00:14:44,120
I've just done that and the police
officers asked me to come with him.

263
00:14:44,280 --> 00:14:46,800
I've got me back up.
I said to him: "fuck off".

264
00:14:46,920 --> 00:14:49,880
I've got now a 90 pound fine.
There you go. Look at that.

265
00:14:50,160 --> 00:14:52,720
Thanks, lads. 90 pound.
Well done.

266
00:14:53,080 --> 00:14:56,600
Yeah, that Guy Ritchie character
was rightly mad about that.

267
00:14:56,840 --> 00:14:59,440
If you are not British
and you're looking at that man,

268
00:14:59,520 --> 00:15:02,280
then at me, and wondering how
we both came from the same island,

269
00:15:02,360 --> 00:15:03,400
let me explain.

270
00:15:03,480 --> 00:15:05,280
British people
come in two variations:

271
00:15:05,360 --> 00:15:07,960
so emotionally stunted that
they're practically comatose

272
00:15:08,040 --> 00:15:10,320
and cheerfully telling
large groups of policemen to:

273
00:15:10,560 --> 00:15:13,760
"Fuck off and do one if you're
gonna take a photo of me face !"

274
00:15:14,080 --> 00:15:16,320
There's absolutely nothing
in between the two.

275
00:15:16,560 --> 00:15:19,240
And the U.K. is by no means alone
in building out a system.

276
00:15:19,320 --> 00:15:22,960
Australia is investing heavily
in a national facial biometric system

277
00:15:23,040 --> 00:15:24,680
called "The Capability",

278
00:15:24,840 --> 00:15:27,400
which sounds like the name
of a Netflix original movie.

279
00:15:27,480 --> 00:15:29,920
Although that's actually perfect
if you want people to notice it,

280
00:15:30,000 --> 00:15:33,000
think: "That seems interesting"
and then forget it ever existed.

281
00:15:33,280 --> 00:15:36,640
And you don't have to imagine
what this technology would look like

282
00:15:36,720 --> 00:15:38,600
in the hands
of an authoritarian government,

283
00:15:38,680 --> 00:15:42,320
because China is unsurprisingly
embracing it in a big way.

284
00:15:43,880 --> 00:15:46,080
We can match every face
with an ID card

285
00:15:46,240 --> 00:15:48,960
and trace all your movements
back one week in time.

286
00:15:49,720 --> 00:15:53,280
We can match your face with your car,
match you with your relatives

287
00:15:53,400 --> 00:15:55,360
and the people
you're in touch with.

288
00:15:55,480 --> 00:15:58,000
With enough cameras, we can
know who you frequently meet.

289
00:15:58,440 --> 00:16:01,280
That is a terrifying
level of surveillance.

290
00:16:01,560 --> 00:16:03,120
Imagine the Eye of Sauron,

291
00:16:03,240 --> 00:16:05,520
but instead of scouring
Middle-earth for the one ring,

292
00:16:05,600 --> 00:16:08,880
he was just really into knowing where
all of his Orcs like to go to dinner.

293
00:16:08,960 --> 00:16:11,880
Some state-funded developers in China
seem weirdly oblivious

294
00:16:11,960 --> 00:16:14,760
to just how sinister
their projects sound.

295
00:16:15,360 --> 00:16:16,840
Skynet. What is that ?

296
00:16:17,160 --> 00:16:21,240
"The Terminator" is
the favorite film of our founder.

297
00:16:21,600 --> 00:16:23,800
So, they used the same name,

298
00:16:24,240 --> 00:16:27,600
but they want to put
something good into this system.

299
00:16:28,440 --> 00:16:32,440
In "The Terminator", Skynet is evil,
rains down death from the sky.

300
00:16:33,800 --> 00:16:37,080
- But in China, Skynet is good.
- Yeah, that's the difference.

301
00:16:37,400 --> 00:16:39,440
That's the difference, is it ?

302
00:16:39,800 --> 00:16:42,360
It's not exactly reassuring
that you called your massive,

303
00:16:42,440 --> 00:16:46,040
all-encompassing AI network
"Skynet, But a Good Version".

304
00:16:46,200 --> 00:16:48,800
It'd be like if "The Today Show"
built a robot journalist

305
00:16:48,880 --> 00:16:50,960
and called it
"Matt Lauer, But Good".

306
00:16:51,040 --> 00:16:52,800
This one's completely different !

307
00:16:52,880 --> 00:16:55,360
Sure, he does also have a button
under his office desk,

308
00:16:55,440 --> 00:16:57,840
but all it does is release
lilac air freshener !

309
00:16:57,960 --> 00:16:59,440
This is the good version.

310
00:16:59,960 --> 00:17:01,920
This technology raises

311
00:17:02,000 --> 00:17:04,880
troubling philosophical questions
about personal freedom.

312
00:17:04,960 --> 00:17:08,560
And right now, there are also some
very immediate, practical issues.

313
00:17:08,640 --> 00:17:11,120
Because even though
it is currently being used,

314
00:17:11,200 --> 00:17:14,440
this technology is still
very much a work in progress.

315
00:17:14,520 --> 00:17:16,320
Its error rate is particularly high

316
00:17:16,400 --> 00:17:18,520
when it comes to matching faces
in real time.

317
00:17:18,600 --> 00:17:21,800
In the U.K., when human rights
researchers watched police

318
00:17:21,880 --> 00:17:23,640
put one such system to the test,

319
00:17:23,720 --> 00:17:28,440
they found that only eight out
of 42 matches were verifiably correct.

320
00:17:28,800 --> 00:17:31,800
That's even before we get into
the fact that these systems

321
00:17:31,880 --> 00:17:35,400
can have some worrying blind spots,
as one MIT researcher found out

322
00:17:35,480 --> 00:17:39,920
when testing out algorithms, including
Amazon's "Rekognition" system.

323
00:17:40,200 --> 00:17:43,880
At first glance, MIT researcher
Joy Buolamwini says

324
00:17:43,960 --> 00:17:47,320
the overall accuracy rate was high,
even though all companies

325
00:17:47,400 --> 00:17:50,240
better detected and identified
men's faces than women's.

326
00:17:50,760 --> 00:17:53,000
But the error rate
grew as she dug deeper.

327
00:17:53,200 --> 00:17:57,760
Lighter male faces were
the easiest to guess the gender on

328
00:17:58,040 --> 00:18:00,480
and darker female faces
were the hardest.

329
00:18:00,680 --> 00:18:03,400
One system couldn't even detect
if she had a face

330
00:18:03,480 --> 00:18:05,800
and the others
misidentified her gender.

331
00:18:06,080 --> 00:18:07,600
White guy, no problem.

332
00:18:08,120 --> 00:18:12,640
Yeah, "White guy, no problem."
The unofficial motto of history.

333
00:18:12,800 --> 00:18:14,600
But it's not like what we needed
right now

334
00:18:14,680 --> 00:18:18,080
was for computers to somehow
find a way to exacerbate the problem.

335
00:18:18,200 --> 00:18:20,360
And it gets worse. In one test,

336
00:18:20,440 --> 00:18:23,640
Amazon's system even failed
on the face of Oprah Winfrey,

337
00:18:23,720 --> 00:18:27,120
someone so recognizable
her magazine only had to type

338
00:18:27,200 --> 00:18:31,080
the first letter of her name
and your brain autocompleted the rest.

339
00:18:31,600 --> 00:18:35,440
A federal study of more than a hundred
facial recognition algorithms

340
00:18:35,520 --> 00:18:37,640
found that Asian
and African American people

341
00:18:37,720 --> 00:18:42,120
were up to 100 times more likely
to be misidentified than white men.

342
00:18:42,320 --> 00:18:45,440
So, that is clearly concerning.
And on top of all this,

343
00:18:45,640 --> 00:18:48,280
some law enforcement agencies
have been using these systems

344
00:18:48,360 --> 00:18:51,240
in ways they weren't
exactly designed to be used.

345
00:18:51,760 --> 00:18:54,840
In 2017, police were looking
for this beer thief.

346
00:18:55,040 --> 00:18:56,920
The surveillance image
wasn't clear enough

347
00:18:57,000 --> 00:18:59,720
for facial recognition software
to identify him.

348
00:18:59,840 --> 00:19:02,800
So instead,
police used a picture of a lookalike,

349
00:19:02,960 --> 00:19:06,000
which happened to be
actor Woody Harrelson.

350
00:19:06,280 --> 00:19:10,600
That produced names of several
possible suspects and led to an arrest.

351
00:19:11,120 --> 00:19:14,720
They used a photo of Woody Harrelson
to catch a beer thief !

352
00:19:14,880 --> 00:19:17,040
And how dare you drag
Woody Harrelson into this ?

353
00:19:17,120 --> 00:19:20,840
This is the man that once got drunk
at Wimbledon in this magnificent hat,

354
00:19:20,920 --> 00:19:23,480
made this facial expression
in the stands, and in doing so,

355
00:19:23,600 --> 00:19:26,280
accidentally made tennis
interesting for a day.

356
00:19:26,360 --> 00:19:29,880
He doesn't deserve prison for that,
he deserves the Wimbledon trophy.

357
00:19:29,960 --> 00:19:32,200
And there have been multiple instances
where investigators

358
00:19:32,280 --> 00:19:35,760
have had such confidence in a match,
they've made disastrous mistakes.

359
00:19:35,840 --> 00:19:38,600
A few years back, Sri Lankan
authorities mistakenly targeted

360
00:19:38,680 --> 00:19:42,720
this Brown University student
as a suspect in a heinous crime,

361
00:19:42,840 --> 00:19:45,120
which made for a pretty awful
finals week.

362
00:19:45,560 --> 00:19:49,200
On the morning of April 25th,
in the midst of finals season,

363
00:19:49,560 --> 00:19:53,040
I woke up in my dorm room
to 35 missed calls,

364
00:19:53,560 --> 00:19:57,240
all frantically informing me
that I had been falsely identified

365
00:19:57,320 --> 00:20:00,720
as one of the terrorists involved
in the recent Easter attacks

366
00:20:00,840 --> 00:20:03,240
in my beloved motherland, Sri Lanka.

367
00:20:03,720 --> 00:20:06,600
That's terrible. Finals week
is already bad enough,

368
00:20:06,680 --> 00:20:10,760
alternating shots of Five Hour Energy
and Java Monster Mean Bean

369
00:20:10,840 --> 00:20:14,520
to remember the differences between
Baroque and Rococo architecture,

370
00:20:14,640 --> 00:20:17,640
without waking up to find out that
you've also been accused of terrorism

371
00:20:17,720 --> 00:20:19,680
because a computer sucks at faces.

372
00:20:20,000 --> 00:20:24,000
On the one hand, these technical issues
could get smoothed out over time.

373
00:20:24,080 --> 00:20:27,400
But, even if this technology
eventually becomes perfect,

374
00:20:27,520 --> 00:20:30,480
we should really be asking ourselves
how much we're comfortable

375
00:20:30,560 --> 00:20:33,680
with it being used, by police,
by governments, by companies,

376
00:20:33,760 --> 00:20:35,320
or indeed by anyone.

377
00:20:35,800 --> 00:20:37,800
We should be asking that right now,

378
00:20:37,880 --> 00:20:40,200
because we're about
to cross a major line.

379
00:20:40,280 --> 00:20:43,800
For years, tech companies approached
facial recognition with caution.

380
00:20:43,880 --> 00:20:48,120
In 2011, the then-chairman of Google
said it was the one technology

381
00:20:48,200 --> 00:20:52,040
the company had held back, because
it could be used "in a very bad way".

382
00:20:52,200 --> 00:20:56,280
And think about that, it was too
Pandora's Box-y for Silicon Valley,

383
00:20:56,440 --> 00:21:00,120
the world's most enthusiastic
Pandora's Box openers.

384
00:21:00,440 --> 00:21:04,080
Even some of the big companies that
developed facial recognition algorithms

385
00:21:04,160 --> 00:21:06,840
have designed it for use
on limited data sets,

386
00:21:06,920 --> 00:21:09,280
like mug shots
or drivers' license photos.

387
00:21:09,360 --> 00:21:11,920
But now,
something important has changed.

388
00:21:12,000 --> 00:21:14,960
And it is because of this guy,
Hoan Ton-That,

389
00:21:15,080 --> 00:21:17,080
and his company, Clearview AI.

390
00:21:17,280 --> 00:21:19,160
I'll let him describe what it does.

391
00:21:19,520 --> 00:21:24,360
Quite simply, Clearview is basically
a search engine for faces.

392
00:21:24,640 --> 00:21:28,400
Anyone in law enforcement
can upload a face to the system

393
00:21:28,560 --> 00:21:32,960
and it finds other publicly available
material that matches that face.

394
00:21:33,320 --> 00:21:36,760
So the key phrase there
is "publicly available material".

395
00:21:36,840 --> 00:21:40,960
Because Clearview says it's collected
a database of three billion images,

396
00:21:41,040 --> 00:21:44,400
that is larger than any other facial
recognition database in the world.

397
00:21:44,480 --> 00:21:47,720
And it's done that by scraping them
from public-facing social media,

398
00:21:47,800 --> 00:21:50,560
like Facebook, Linkedin, Twitter
and Instagram.

399
00:21:50,640 --> 00:21:53,160
So, for instance, Clearview's system
would theoretically include

400
00:21:53,240 --> 00:21:57,000
this photo of Ton-That
at what appears to be Burning Man,

401
00:21:57,080 --> 00:21:59,680
or this one, of him wearing a suit
from the exclusive

402
00:21:59,760 --> 00:22:02,360
"Santa Claus After Dark Collection"
at Men's Wearhouse.

403
00:22:02,440 --> 00:22:05,640
And this very real photo of him
shirtless and lighting a cigarette

404
00:22:05,720 --> 00:22:09,280
with blood-covered hands,
his profile photo on Tidal,

405
00:22:09,360 --> 00:22:11,840
because yes, of course
he's also a musician.

406
00:22:11,920 --> 00:22:14,480
I can only assume that that's the cover
of an album called:

407
00:22:14,560 --> 00:22:17,840
"Automatic Skip If This Ever
Comes Up On A Pandora Station".

408
00:22:17,920 --> 00:22:21,320
Ton-That's willingness to do
what others have not been willing to do

409
00:22:21,440 --> 00:22:24,160
and that is scrape
the whole internet for photos,

410
00:22:24,240 --> 00:22:28,360
has made his company a genuine
game-changer in the worst way.

411
00:22:28,760 --> 00:22:32,160
Watch as he impresses a journalist
by running a sample search.

412
00:22:32,320 --> 00:22:37,000
So, here's the photo you uploaded
of me. A headshot from CNN.

413
00:22:37,960 --> 00:22:39,880
So, first few images it's found,

414
00:22:39,960 --> 00:22:42,680
it's found a few different versions
of that same picture.

415
00:22:42,760 --> 00:22:45,800
But now as we scroll down,
we're starting to see pictures of me

416
00:22:45,880 --> 00:22:48,480
that are not
from that original image.

417
00:22:48,960 --> 00:22:51,040
My god.

418
00:22:51,120 --> 00:22:57,000
So, this photograph is from my local
newspaper where I lived in Ireland.

419
00:22:58,040 --> 00:23:01,760
And this photo would've been taken
when I was like 16.

420
00:23:05,680 --> 00:23:07,000
That's crazy.

421
00:23:07,320 --> 00:23:08,560
Yeah, it is.

422
00:23:08,960 --> 00:23:12,760
If there is an embarrassing photo
of you from when you were a teenager,

423
00:23:12,840 --> 00:23:14,120
don't run away from it.

424
00:23:14,200 --> 00:23:17,600
Make it the center of your TV show's
promotional campaign and own it.

425
00:23:17,720 --> 00:23:21,200
Use the fact that your teenage years
were a hormonal Stalingrad.

426
00:23:21,320 --> 00:23:22,440
Harness the pain.

427
00:23:22,800 --> 00:23:24,960
But the notion that someone
can take your picture

428
00:23:25,040 --> 00:23:27,640
and immediately find out
everything about you is alarming enough

429
00:23:27,720 --> 00:23:31,640
even before you discover
that over 600 law enforcement agencies

430
00:23:31,720 --> 00:23:33,840
have been using
Clearview's service.

431
00:23:34,000 --> 00:23:37,480
And you're probably in that database,
even if you don't know it.

432
00:23:37,600 --> 00:23:39,760
If a photo of you has been
uploaded to the internet,

433
00:23:39,840 --> 00:23:41,920
there is a decent chance
that Clearview has it.

434
00:23:42,000 --> 00:23:44,440
Even if someone uploaded it
without your consent,

435
00:23:44,560 --> 00:23:48,040
even if you untagged yourself or later
set your account to "private".

436
00:23:48,120 --> 00:23:49,200
If you think:

437
00:23:49,280 --> 00:23:52,800
"Isn't this against the terms
of service for internet companies ?"

438
00:23:52,880 --> 00:23:55,920
Clearview actually received
cease-and-desist orders

439
00:23:56,000 --> 00:23:58,720
from Twitter, YouTube
and Facebook earlier this year.

440
00:23:58,840 --> 00:24:01,200
But it has refused to stop, arguing

441
00:24:01,280 --> 00:24:05,240
that it has a First Amendment right
to harvest data from social media.

442
00:24:05,360 --> 00:24:08,560
Which is just not at all
how the First Amendment works.

443
00:24:08,760 --> 00:24:11,040
You might as well argue that you have
an Eighth Amendment right

444
00:24:11,120 --> 00:24:12,920
to dress up rabbits
like John Lennon.

445
00:24:13,000 --> 00:24:16,920
That amendment does not cover
what I think you think it does.

446
00:24:17,400 --> 00:24:20,440
And yet, Ton-That insists
that this was all inevitable,

447
00:24:20,560 --> 00:24:23,760
so we should all frankly be glad
that he's the one who did it.

448
00:24:24,080 --> 00:24:25,280
I think the choice now

449
00:24:25,360 --> 00:24:28,280
is not between, like, no facial
recognition and facial recognition.

450
00:24:28,360 --> 00:24:33,240
It's between bad facial recognition
and responsible facial recognition.

451
00:24:33,320 --> 00:24:35,080
And we want to be
in the responsible category.

452
00:24:35,440 --> 00:24:38,200
Well, sure, you want to be.
But are you ?

453
00:24:38,600 --> 00:24:40,440
Because there are
a lot of red flags here.

454
00:24:40,560 --> 00:24:44,200
Apps he developed before this
included one called "Trump Hair",

455
00:24:44,320 --> 00:24:46,760
which would just add Trump's hair
to a user's photo

456
00:24:46,840 --> 00:24:50,200
and another called "ViddyHo"
that phished its own users,

457
00:24:50,280 --> 00:24:52,920
tricking them into sharing access
to their Gmail accounts

458
00:24:53,000 --> 00:24:54,680
and then spamming
their contacts.

459
00:24:54,760 --> 00:24:58,000
I'm not sure that I would want
to trust my privacy to this guy.

460
00:24:58,120 --> 00:25:00,960
If I was looking for someone to build
an app that let me

461
00:25:01,040 --> 00:25:04,520
put Ron Swanson's mustache on my
face as my account was drained,

462
00:25:04,600 --> 00:25:06,880
then he'd be the top of my list.

463
00:25:07,280 --> 00:25:09,960
Despite Clearview's
repeated reassurances

464
00:25:10,080 --> 00:25:13,080
that its product is intended
only for law enforcement,

465
00:25:13,160 --> 00:25:15,240
as if that is inherently a good thing,

466
00:25:15,320 --> 00:25:18,120
he's already put it in a lot
of other people's hands.

467
00:25:18,240 --> 00:25:20,920
In addition to users
like the DEA and the FBI,

468
00:25:21,040 --> 00:25:24,760
he's also made it available
to employees at Kohl's, Walmart

469
00:25:24,880 --> 00:25:29,720
and Macy's, which has alone completed
more than 6 000 facial searches.

470
00:25:29,920 --> 00:25:33,560
And it gets worse, they've reportedly
tried to pitch their service

471
00:25:33,680 --> 00:25:37,240
to congressional candidate
and white supremacist Paul Nehlen,

472
00:25:37,320 --> 00:25:40,400
suggesting they could help him
use "unconventional databases"

473
00:25:40,480 --> 00:25:44,440
for "extreme opposition research",
which is a terrifying series of words

474
00:25:44,520 --> 00:25:47,200
to share a sentence with
"white supremacist".

475
00:25:47,560 --> 00:25:50,480
Clearview says that
that offer was "unauthorized",

476
00:25:50,560 --> 00:25:53,760
but when questioned about who else
he might be willing to work with,

477
00:25:53,840 --> 00:25:56,320
Ton-That's answer
hasn't been reassuring.

478
00:25:56,600 --> 00:26:00,040
There's countries I would never sell to
that are very adverse to the U.S.

479
00:26:00,120 --> 00:26:02,360
- For example ?
- Like China and Russia.

480
00:26:02,720 --> 00:26:07,000
Iran, North Korea. Those are the things
that are definitely off the table.

481
00:26:08,040 --> 00:26:09,760
What about countries that think

482
00:26:09,880 --> 00:26:12,760
that being gay should be illegal,
it's a crime ?

483
00:26:14,240 --> 00:26:18,280
Like I said, we want to make sure
that we do everything correctly,

484
00:26:18,400 --> 00:26:20,240
mainly focus on the U.S.
and Canada.

485
00:26:20,320 --> 00:26:22,520
And the interest has been
overwhelming.

486
00:26:22,640 --> 00:26:25,640
Just so much interest,
we're taking it one day at a time.

487
00:26:25,960 --> 00:26:28,440
Yeah,
that's not terribly comforting !

488
00:26:28,720 --> 00:26:31,760
When you ask a farmer if he'd
let foxes into the hen house,

489
00:26:31,880 --> 00:26:33,560
the answer you hope for
is "No",

490
00:26:33,640 --> 00:26:36,360
not: "The interest from foxes
has been overwhelming,"

491
00:26:36,440 --> 00:26:39,800
"just so much interest,
we're taking it one day at a time."

492
00:26:40,360 --> 00:26:44,680
Reporters for Buzzfeed have found
that Clearview has offered its services

493
00:26:44,760 --> 00:26:48,160
to entities in Saudi Arabia
and the United Arab Emirates,

494
00:26:48,240 --> 00:26:51,360
countries that view human rights
laws with the same level of respect

495
00:26:51,440 --> 00:26:54,480
that Clearview seems to have
for Facebook's terms of service.

496
00:26:54,560 --> 00:26:57,440
So, facial recognition technology
is already here.

497
00:26:57,520 --> 00:26:59,960
The question is,
what can we do about it ?

498
00:27:00,160 --> 00:27:03,600
Some are trying to find ways
to thwart the cameras themselves.

499
00:27:04,080 --> 00:27:07,280
Hi guys. It's me, Jillian, again,
with a new makeup tutorial.

500
00:27:07,400 --> 00:27:09,960
Today's topic
is how to hide from cameras.

501
00:27:10,560 --> 00:27:13,800
First, that's probably
not a scalable solution,

502
00:27:13,920 --> 00:27:16,640
and second, I'm not sure if that
makes you less identifiable

503
00:27:16,720 --> 00:27:19,400
or the most identifiable
person on earth.

504
00:27:19,680 --> 00:27:21,400
Officers are on the lookout
for a young woman,

505
00:27:21,520 --> 00:27:24,800
dark hair, medium build, looks like
a mime who went through a shredder.

506
00:27:24,920 --> 00:27:26,960
What we really need to do

507
00:27:27,040 --> 00:27:29,800
is to put limits on how
this technology can be used.

508
00:27:29,880 --> 00:27:32,240
Some locations
have laws in place already.

509
00:27:32,440 --> 00:27:35,320
San Francisco banned
facial recognition last year,

510
00:27:35,400 --> 00:27:39,320
but the scope of that is limited
to city law enforcement.

511
00:27:39,400 --> 00:27:42,760
It doesn't affect state and
federal use or private companies.

512
00:27:42,840 --> 00:27:46,880
Illinois has a law requiring
companies to obtain written permission

513
00:27:46,960 --> 00:27:49,520
before collecting fingerprints,
facial scans

514
00:27:49,640 --> 00:27:53,400
or other identifying biological
characteristics and that is good.

515
00:27:53,480 --> 00:27:59,080
We also need a comprehensive,
nation-wide policy. We need it now.

516
00:27:59,280 --> 00:28:03,800
There are worries that it is being used
in the protests that we are seeing now.

517
00:28:03,880 --> 00:28:05,680
And the good news
is that just this week,

518
00:28:05,760 --> 00:28:09,200
thanks to those protests
and to years of work by activists,

519
00:28:09,280 --> 00:28:12,280
some companies
did pull back from facial recognition.

520
00:28:12,480 --> 00:28:15,680
IBM says they'll no longer
develop facial recognition.

521
00:28:15,840 --> 00:28:20,160
Amazon said it was putting a one-year
hold on working with law enforcement.

522
00:28:20,240 --> 00:28:23,360
Microsoft said it wouldn't sell
its technology to police

523
00:28:23,440 --> 00:28:25,160
without federal regulation.

524
00:28:25,360 --> 00:28:28,960
But, there is nothing to stop those
companies from changing their mind

525
00:28:29,080 --> 00:28:31,240
if people's outrage dies down.

526
00:28:31,840 --> 00:28:35,560
While Clearview says
it's canceling its private contracts,

527
00:28:35,680 --> 00:28:38,560
it's also said it will keep working
with the police,

528
00:28:38,640 --> 00:28:42,440
just as it will keep harvesting
your photos from the internet.

529
00:28:42,840 --> 00:28:46,160
If Clearview is gonna keep grabbing
our photos, at the very least,

530
00:28:46,240 --> 00:28:49,400
there may be a way to let them know
what you think about that.

531
00:28:49,520 --> 00:28:51,920
So, the next time you feel
the need to upload a photo,

532
00:28:52,000 --> 00:28:54,840
maybe throw in an extra one
for them to collect.

533
00:28:54,920 --> 00:28:58,200
Maybe hold up a sign that says:
"These photos were taken unwillingly"

534
00:28:58,280 --> 00:29:00,200
"and I'd rather
you not be looking at them."

535
00:29:00,280 --> 00:29:03,600
Or, if that feels too complicated,
just: "Fuck Clearview".

536
00:29:03,680 --> 00:29:05,880
That really does
get the message across.

537
00:29:05,960 --> 00:29:09,040
These photos are often being searched
by law enforcement,

538
00:29:09,120 --> 00:29:10,600
so take this opportunity

539
00:29:10,680 --> 00:29:13,400
to talk to the investigators
looking through your photos.

540
00:29:13,480 --> 00:29:16,280
Maybe something like:
"I don't look like Woody Harrelson,"

541
00:29:16,400 --> 00:29:19,440
"but while I have your attention,
defund the police."

542
00:29:19,560 --> 00:29:23,720
Whatever you feel is most important
to tell them, you should put on a sign.

543
00:29:24,040 --> 00:29:27,560
That's our show. We'll see you
next week. Good night !

544
00:29:29,880 --> 00:29:32,400
"I Am Not Steve Mnuchin".
